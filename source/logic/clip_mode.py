import cv2
import numpy as np
import configparser
import sys
from PIL import Image
from typing import List, Tuple, Optional,Dict
import torch
import open_clip

import sys
from pathlib import Path  
root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(root))

from common.config_manager import ConfigManager


MY_TEXT_LABELS = [
        "T-shirt", "black clothing", "winter clothing", "summer clothing",
        "plush toy", "down jacket", "wallet", "sweater", "leggings",
        "underwear", "shoe", "dress"
    ]



class CaseSensitiveConfigParser(configparser.ConfigParser):
    """ç»§æ‰¿ConfigParserï¼Œè¦†ç›–optionxformæ–¹æ³•ä»¥ä¿æŒé€‰é¡¹ååŸæ ·"""
    def optionxform(self, optionstr: str) -> str:
        return optionstr  # ä¸å°†é€‰é¡¹åè½¬æ¢ä¸ºå°å†™
    

class clipClass:
    def __init__(self, model_name='ViT-SO400M-16-SigLIP2-512', pretrained='webli',data=None):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨ï¼ŒåŠ è½½æ¨¡å‹ã€åˆ†è¯å™¨ã€é¢„å¤„ç†å’Œæ–‡æœ¬æ ‡ç­¾ã€‚

        Args:
            model_name (str): æ¨¡å‹åç§°ï¼Œå¦‚ 'ViT-SO400M-16-SigLIP2-512'
            pretrained (str): é¢„è®­ç»ƒæƒé‡ï¼Œå¦‚ 'webli'
            text_labels (list[str]): æ–‡æœ¬æ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚ ["T-shirt", "dress", ...]
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)
        self.model.eval()
        self.cfg=ConfigManager()
        self.model = self.model.to(self.device)
        if data is not None:
            self.label_mapping = data
        else:
            self.label_mapping=self.cfg.get("clip_mode","labels")
       
        # è·å–åˆ†è¯å™¨
        self.tokenizer = open_clip.get_tokenizer(model_name)
        self.text_labels= [it for it in self.label_mapping]

        self.text_tokens = self.tokenizer(self.text_labels).to(self.device)  # tokenize å¹¶ç§»è‡³è®¾å¤‡

    def predict(self, image: Image.Image) -> Tuple[str, float]:
        """
        å¯¹ä¸€å¼  PIL.Image å›¾ç‰‡å¯¹è±¡è¿›è¡Œé¢„æµ‹ï¼Œè¿”å›æœ€å¯èƒ½çš„æ–‡æœ¬æ ‡ç­¾åŠç½®ä¿¡åº¦ï¼ˆç™¾åˆ†æ¯”ï¼‰ã€‚

        Args:
            image (PIL.Image.Image): è¾“å…¥çš„å›¾ç‰‡ï¼Œå¿…é¡»æ˜¯ PIL.Image å¯¹è±¡ï¼Œä¸”æœ€å¥½æ˜¯ RGB æ¨¡å¼

        Returns:
            Tuple[str, float]: (é¢„æµ‹çš„æ–‡æœ¬æ ‡ç­¾, ç½®ä¿¡åº¦ç™¾åˆ†æ¯”)ï¼Œä¾‹å¦‚ ("T-shirt", 95.67)
        """
        if not isinstance(image, Image.Image):
            raise TypeError(f"è¾“å…¥å¿…é¡»æ˜¯ PIL.Image å¯¹è±¡ï¼Œä½†ä¼ å…¥çš„æ˜¯ {type(image)}")

        # ç¡®ä¿å›¾ç‰‡æ˜¯ RGBï¼ˆå¦‚æœæ˜¯ RGBA æˆ– L ç­‰æ ¼å¼ï¼Œå¯èƒ½ä¼šæŠ¥é”™ï¼‰
        if image.mode != 'RGB':
            image = image.convert('RGB')

        try:
            # é¢„å¤„ç†å›¾ç‰‡
            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)  # [1, 3, H, W]

            # æ¨¡å‹æ¨ç†
            with torch.no_grad(), torch.autocast(device_type="cuda" if self.device == "cuda" else "cpu"):
                image_features = self.model.encode_image(image_tensor)
                text_features = self.model.encode_text(self.text_tokens)

                # å½’ä¸€åŒ– -> ä½™å¼¦ç›¸ä¼¼åº¦ -> softmax æ¦‚ç‡
                image_features /= image_features.norm(dim=-1, keepdim=True)
                text_features /= text_features.norm(dim=-1, keepdim=True)

                text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)

            probs = text_probs[0].cpu().numpy()
            predicted_idx = probs.argmax()
            predicted_label = self.text_labels[predicted_idx]
            confidence = float(probs[predicted_idx])*100  # å¦‚ 95.67

            return predicted_label, confidence

        except Exception as e:
            raise RuntimeError(f"å›¾ç‰‡æ¨ç†è¿‡ç¨‹ä¸­å‡ºé”™ï¼š{e}")

    def match_clip(self,frame0: np.ndarray,frame1:np.ndarray) -> Tuple[np.ndarray, str, float, int]:
        """
        å•å¼  BGR å›¾ -> CLIP åˆ†ç±»ç»“æœ
        :param frame:        BGR å›¾
        :param classifier:   ä½ è‡ªå·±çš„ ClipClassifier å®ä¾‹
        :param label_mapping:å¤–éƒ¨å¯å¤ç”¨ç¼“å­˜ï¼Œä¼  None åˆ™å†…éƒ¨è‡ªåŠ¨åŠ è½½
        :return:
            vis      : RGB å›¾
            label    : æœ€ä½³æ ‡ç­¾
            conf     : ç½®ä¿¡åº¦
            label_id : å¯¹åº” ID
        """
        #TODO :è¡¥å……ä¸¤å¼ å›¾ç‰‡çš„é€»è¾‘


        vis = cv2.cvtColor(frame0, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(vis)

        label, conf = self.predict(pil_image)   # è¿”å› (str, float)
        label_id = self.label_mapping.get(label, -1)       # æ‰¾ä¸åˆ°ç»™ -1
        return vis, label, conf, label_id


if __name__ == "__main__":
    import sys
    data={
            "T-shirt": 0,
            "black clothing": 1,
            "winter clothing": 2,
            "summer clothing": 3,
            "plush toy": 4,
            "down jacket": 5,
            "wallet": 6,
            "sweater": 7,
            "leggings": 8,
            "underwear": 9,
            "shoe": 10,
            "dress": 11
        }
    classifier = clipClass()
    img_path = r"C:\Users\14676\Desktop\new_env\shoe\imgs\2025-10-16-13-43-33.png"
    frame = cv2.imread(img_path)
    frame0 = frame.copy()
    if frame is None:
        print("å›¾ç‰‡æ²¡è¯»è¿›æ¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–æ–‡ä»¶æ˜¯å¦æŸå")
        sys.exit()
    
    print("å›¾ç‰‡å°ºå¯¸:", frame.shape[:2])

    vis, label, conf, label_id = classifier.match_clip(frame0,frame)
    print(f"CLIP é¢„æµ‹ -> label={label}  conf={conf:.3f}  id={label_id}")

    cv2.namedWindow("clip_result", cv2.WINDOW_NORMAL)
    cv2.imshow("clip_result", cv2.cvtColor(vis, cv2.COLOR_RGB2BGR))
    print("æŒ‰ä»»æ„é”®å…³é—­çª—å£...")
    cv2.waitKey(0)
    cv2.destroyAllWindows()
# if __name__ == "__main__":
#     label = load_clip_label_mapping()
#     print(label)
#     text = []
#     for it in label :

#         text.append(it)
#     print(text)

